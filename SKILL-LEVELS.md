### Learning Path 1: Fundamentals and Correct Thinking as an Applied AI Engineer
This path focuses on building a strong base in core "Must" skills, operating principles, and foundational concepts. It emphasizes outcomes-oriented thinking, reproducibility, safety by default, and ruthless simplicity. You'll learn to prioritize data quality, basic ML/DL architectures, and end-to-end ownership of simple AI services (e.g., prompting, basic RAG, and deployment). This covers essential mathematical/ML foundations, software engineering basics, data engineering lite, classical ML, DL architectures (vision/NLP basics), training efficiency fundamentals, LLM/RAG/agent basics, vision inference, serving reliability, experimentation/eval fundamentals, security/privacy basics, product framing, cloud/hardware basics, and collaboration. Practice with Python, SQL, NumPy, SciPy, PyTorch, scikit-learn, HF Transformers, FastAPI, Docker, and basic observability tools.

#### Step 1: Master Operating Principles and Meta Skills
- Study outcomes > models, data first, ruthless simplicity, reproducibility (one-command runs, seeds, data snapshots), observability (metrics/logs/traces/evals before scaling), safety/compliance by default (PII, licensing, security), FinOps (\$ per req/token), writing it down (design docs, ADRs, runbooks, postmortems, model/data cards), version everything (data, code, models, prompts, retrievers, indices, evals, policies), and provenance/integrity (attest/verify artifacts, immutable IDs, audit trails).
- Build proof bundles: Write a design doc for a simple AI task, including ablations with cost/latency, runbooks, cost dashboards, postmortems, and registries with verified artifacts/provenance.
- Practice: Implement a baseline ML task (e.g., classification) with full reproducibility, including seed control, numeric parity checks, and CI gates.

#### Step 2: Build Mathematical, ML, and Software Foundations
- Linear algebra/calculus: Conditioning, optimizer schedules, normalization/initialization, FP16/bfloat16 guardrails; implement custom loss + gradient, autodiff tests, NaN sentinels.
- Probability/statistics: Calibration, CIs vs significance, bootstrap, uncertainty surfacing.
- Classical ML: Linear/logistic, trees/GBMs, SVMs, clustering, naïve Bayes; robust CV, leakage prevention, class imbalance, calibration; know when GBM beats Transformer for tabular.
- Software: Python/SQL excellence (type hints, asyncio, vectorization, SQL windows/CTEs, schema contracts); code quality (clean architecture, packaging, profiling with flamegraphs/speedups); tooling (tests/unit/integration/property, lint/format, typing gates, CI, deterministic builds, SBOM awareness).
- Data engineering: Ingestion/storage (data contracts, Parquet/Avro, point-in-time correctness, simple batch/cron); quality/governance (schema/range/distribution checks, drift monitors, PII discovery/redaction, contract tests failing CI); labeling (active/weak supervision, HITL, programmatic labeling, IAA/adjudication, golden-set curation).
- DL architectures: Vision (CNNs, ViTs, detection/segmentation, augmentations, small-data transfer, metric learning contrastive/CLIP-style); NLP/LLMs (tokenization SentencePiece/BPE/unigram, Transformers, embeddings, IR/ranking, summarization/QA/NER, tokenizer throughput/prompt-length budgeting).
- Training/efficiency: Performance training (AMP/bfloat16, clipping, schedulers, checkpointing, dataloader throughput); PEFT/compression (LoRA/QLoRA/PEFT, distillation, pruning/sparsity, memory-latency trade-offs); cost awareness (utilization, bandwidth, pricing, \$ per achieved metric dashboards).
- Stack practice: Use NumPy/SciPy/PyTorch/scikit-learn/statsmodels/pymc; XGBoost/LightGBM/CatBoost/Optuna; HF Transformers/Diffusers/Lightning/Accelerate/TorchScript/ONNX.

#### Step 3: Apply to Core AI Systems with Safety and Eval
- LLMs/RAG/Agents: Prompting (roles, few-shot/CoT, tool use, routing, JSON/schema outputs, deterministic parsing, prompt registry + hashing/verification); secrets/context hygiene (secret-scoped prompts, context-leak controls, privacy budgets for logs); RAG (chunking with overlap, hybrid BM25+ANN, reranking, embeddings hygiene, cache taxonomy with eviction/validation, retriever/index versioning, joint rollback); RAG metrics (support-coverage, faithfulness@k, answerable vs unanswerable + abstention, CI regression gates); judges/rerankers (versions, slice drift monitoring, rollback plans, cost/latency SLOs, log judge rationales); agents (planning, approval gates, tool scopes, state/memory, idempotent retries, DLQs, sandboxing, HITL controls); efficient inference (KV-cache mgmt, paged attention, quantization AWQ/GPTQ/GGUF/LLM.int8, speculative decoding, batching/streaming, cost/latency SLOs); SLM basics (model zoo/licenses 3-7B, CPU/single-GPU serving llama.cpp/Ollama vs vLLM/TGI, GGUF/AWQ/GPTQ trade-offs, tokenizer throughput, rope/pos-scaling).
- Vision: Transfer learning (CNN/ViT), detection/segmentation basics, augmentations, OCR basics, ONNX/TensorRT export; export/runtimes (TorchScript/ONNX, TensorRT, OpenVINO, batch/stream video, pre/post-processing correctness).
- Serving/reliability: APIs/services (streaming, admission control, rate limiting, retries/timeouts/circuit breakers, schema contracts); rollouts (shadow/canary/blue-green, feature flags, model+prompt+index+judge/policy registry integration, one-command rollback); observability (traces/metrics/logs, golden signals, token/req cost, evals in prod, data slices/drift, judge/policy decision logs with sampled human audits); capacity/cost (autoscaling, throughput models Little’s Law, batching, queueing, P90/P99 budgets, SLOs/error budgets); DR/continuity (backup/restore for models/vectors/prompts/policies, restore drills, key rotation, degrade modes); vendor dependency (quotas/price alarms, multi-vendor abstraction, automatic cut-over within cost ceilings).
- Experimentation/eval: Instrumentation/analytics (event schemas, stable IDs, sampling, retention/cohorts, guardrail metrics, novelty effects/long-horizon backtests, data storytelling with one chart tying latency/cost/quality to KPI); model eval (stratified sets, per-slice metrics, calibration, fairness/robustness, versioned datasets, CI gates); LLM-judge hygiene (judge calibration, position-bias mitigation, human agreement checks, CIs on win-rates, log judge rationales for audit).
- Security/privacy: Security for ML (STRIDE + ML risks, supply chain SBOM/SLSA targets, artifact verification, prompt injection/tool SSRF, sandboxing, egress allowlists); abuse/fraud (rate-limit evasion, token-abuse detection, tenant isolation tests); privacy (PII discovery/redaction, data minimization, retention, DP basics, federated patterns, privacy budgets for logs); governance (model/dataset cards, approvals/audit trails, RBAC, red-teaming, incident response, EU AI Act risk tiers/obligations, NIST AI RMF/ISO 23894 awareness); licensing/IP (CC-BY-SA, non-commercial datasets, field-of-use, scraping T&Cs, proof via license inventory + CI checks).
- Product/UX: Problem framing (constraints to objective, success metrics, risks, buy vs build, de-scope); HITL (reviewer UX, escalation, feedback capture, queue design, incentives); UX writing (response tone, opt-outs/consent, streaming UX); fairness (alerts on slice regressions, mitigation playbooks, human review loops); judge/feedback observability (disagreement rates, slice drift, human audits, rollback plan).
- Cloud/hardware/edge: Cloud (IAM least-privilege, VPCs/subnets, SGs, storage classes/lifecycle, quotas, cost allocation/chargeback); accelerators (GPU/TPU fundamentals, utilization vs latency, TensorRT-LLM/vLLM/ONNX Runtime, memory hygiene, pinned memory); edge/mobile (model conversion ONNX to TFLite/Core ML/HF to gguf, PTQ/QAT, thermals/throttling, offline modes, acceptance tests p95 ≤ X ms on device, update/signing protocol, hardware delegates NNAPI/Metal/ANE/NPU, WebGPU/WebNN).
- Collaboration: Cross-functional fluency (product/data/platform/legal/security, shared SLOs); documentation (READMEs, runbooks, model/data cards, diagrams).
- Stack: Languages (Python/SQL/Bash, Git/GitHub, tests/lint/format/typing, packaging); data (S3/GCS/ADLS, Parquet, light schedulers with contract tests); modeling (PyTorch, scikit-learn, XGBoost/LightGBM, MLflow/W&B); GenAI/RAG (hybrid + rerank, span citations, evals/guardrails, judge hygiene, secret-scoped prompts, cache taxonomy); serving (Docker, FastAPI/gRPC, basic k8s, registry for model/prompt/retriever/index/policy, Prometheus/Grafana/OTel, artifact verification per Release & Provenance Policy); edge/SLMs (llama.cpp/Ollama, gguf conversion, TFLite/Core ML, ONNX Runtime Mobile, quantization int8/int4); security (Vault/Secrets Manager, IAM/OPA basics, PII redaction, SLSA baseline, privacy budgets for logs, abuse/fraud protections, tenant isolation); prototyping (Jupyter, Streamlit/Gradio, matplotlib/Plotly).
- Project: Build a simple LLM-based app (e.g., RAG Q&A) with full versioning, evals, safety checks, and deployment to edge device, documenting with ADRs and cost analysis.

### Learning Path 2: Expansion on Fundamentals – Intermediate Skills and Systems Integration
Building on Path 1, this path advances to "Strong" skills, emphasizing integration across tracks (A-F at L3-L4), deeper DL/multimodal, training/scaling, LLM lifecycle, vision optimization, inference serving, experimentation design, security/privacy depth, product change management, edge strategies, and leadership basics. Focus on owning portfolios with joint rollbacks, multi-vendor fallbacks, and SLM-first approaches. Incorporate learning theory, signal processing, concurrency, streaming data, multimodal VLMs, distributed training awareness, agent orchestration, vision-RAG, client-server UX, A/B testing, content provenance, explainability, and tech stacks like TypeScript, C++/Rust/Go, dbt, Lightning/Accelerate, LangChain/LlamaIndex/LangGraph, vLLM/TensorRT-LLM, DP/Opacus.

#### Step 1: Deepen Foundations and Integrate ML Systems
- Learning theory: Bias-variance, regularization, ablations (data/model/training).
- Information/signal: Filtering, spectral methods, compression, sampling.
- Time-series/causality: Stationarity, leakage traps, proper CV, counterfactuals, policy-aware metrics (awareness to strong).
- Software: Concurrency (async vs threads/procs, queues/backpressure, idempotency, graceful shutdown); determinism/repro (tokenizer/version-skew tests, quantized-vs-FP regression harness, GPU/CPU parity in CI).
- Data: Streaming/near-real-time (Kafka/PubSub basics, idempotent consumers, backfills/replay); pipelines (idempotency, retries, lineage, prefer managed services).
- DL: Multimodal/VLM/MLLM (CLIP/SigLIP encoders, image-text contrastive, BLIP/LLaVA adapters, grounding for RAG); speech/audio/time-series DL (awareness).
- Training/scaling: Distributed (DDP/FSDP/ZeRO, failure recovery, gradient checkpointing awareness to strong); schedulers/clusters (SLURM/k8s jobs, preemption-safe, spot-aware); accelerators (Triton-lang/kernels awareness, prioritize ONNX/TensorRT, vLLM/TensorRT-LLM, profiler literacy).
- LLM lifecycle: SFT vs RLHF/RLAIF/DPO (awareness of methods), safety tuning, eval gates, license/contamination hygiene.

#### Step 2: Advance GenAI, Vision, and Serving
- LLMs/RAG/Agents: Agents/orchestration (planning, approval gates, tool scopes, state/memory, idempotent retries, DLQs, sandboxing, HITL controls – deepen); SLM specialization (CPU/single-GPU serving, GGUF/AWQ/GPTQ trade-offs, tokenizer throughput, rope/pos-scaling).
- Vision: PTQ/QAT, TensorRT optimization, video sampling/batching, multi-task heads; VLMs (CLIP-style for retrieval); optimization (PTQ/QAT, layer fusion, mixed precision, NMS/ROI ops, CPU offload); serving (REST/gRPC, admission control, rate limits, streaming for camera feeds, GPU affinity/NUMA, multi-camera batching, real-time QoS); vision-RAG (image embeddings CLIP/SigLIP, hybrid retrieval, cross-modal reranking, grounded captions); MLLMs (image-text adapters, prompt templates for visual QA, citation spans with coordinates).
- Inference/serving: Client-server latency/UX (TTFT/TBT budgeting, RUM when owning UI, streaming UX, a11y, error/uncertainty messaging); optimization (ONNX/AOT, fusion, pinned memory/NUMA, KV paging, tokenizer throughput).
- Edge: Distillation/quantization trade-offs (AWQ/GPTQ/GGUF/LLM.int8/4-bit), KV-cache mgmt on device, packaging/updates, privacy-first logging; speculative decoding with draft models, KV-cache residency tuning, tokenizer throughput fixes, CPU affinity, WebGPU (strongen).
- Search/retrieval: BM25/Query-Likelihood + embeddings, two-tower vs cross-encoder, ANN (HNSW/IVF), metrics (Recall@k/NDCG) + counterfactual IPS, dedupe/normalize/chunking with overlap; hybrid tuning, reranker selection, exploration budgets, freshness policies, per-slice retrieval metrics.

#### Step 3: Integrate Eval, Security, Product, and Leadership
- Experimentation: Experiment design (metrics trees, traffic allocation, power/duration, sequential/non-inferiority tests, ramp policies); A/B & beyond (CUPED/CUPAC, long-tail monitoring, novelty decay).
- Security/privacy: Content provenance (detect/log, trust policy, surface to users); DP/Opacus, Confidential VMs.
- Product/UX: Explainability/trust (global/local explainers, uncertainty surfacing, citations in RAG, clear error/uncertainty messaging); change management (on-call, postmortems, versioned releases).
- Collaboration/leadership: Technical leadership (roadmaps, ADRs, design reviews, mentoring, risk burndown, cross-team alignment).
- Stack: Languages (TypeScript/Next.js for agent UIs awareness, C++/Rust/Go for perf, Poetry/pip-tools, GitHub Actions); data (dbt, Great Expectations/Soda); modeling (Lightning/Accelerate, Optuna, FSDP/DeepSpeed when scale demands); GenAI (LangChain/LlamaIndex/LangGraph, Milvus/Weaviate/Pinecone, Redis, streaming UX); serving (vLLM/TensorRT-LLM, TVM/ONNX Runtime, RUM when applicable); edge (MLC-LLM, ExecuTorch awareness); security (DP/Opacus, Confidential VMs, content provenance policy); prototyping (product analytics & conditional RUM dashboards).
- Project: Extend Path 1 app to include multimodal RAG, agent tools, edge optimization, A/B evals, and security audits, with joint rollback drills and design reviews.

### Learning Path 3: Advanced Expansion – Leadership, Scaling, and Specialization
Building on Paths 1-2, this path targets "Awareness" to "Strong" for senior/staff levels (L4-L6 across tracks A-F), focusing on org-wide strategies, advanced reliability, SLM/edge programs, perception stacks, retriever architectures, governance, and full tech stacks. Emphasize leading portfolios with multi-vendor cut-overs, trust/safety postures, and specialized domains like RL awareness, JAX, CrewAI/AutoGen/Guidance, KServe/BentoML/Seldon, Helm/Argo, Kafka/Kinesis/PubSub, Airflow/Prefect/Dagster.

#### Step 1: Scale Systems and Reliability
- Applied ML: Data quality, contract tests, leakage-free CV, GBM/basic DL, evals-as-code, KPI mapping; experiment design, per-slice metrics, online/offline linkage, guardrails; leads cross-team metric trees, launch guardrails, fairness/long-tail playbooks; org-wide standards, strategy/portfolio bets tied to KPIs.
- Platform/reliability: CI/CD, containers, FastAPI/gRPC, basic k8s, observability, canary/rollback, artifact verification via Release & Provenance Policy; cost dashboards, rate limits, SLOs/error budgets, multi-model routing, multi-vendor fallbacks; multi-region patterns, DR drills, supply-chain posture, encrypted stores, privacy budgets; company-wide reliability/safety strategy and audits.
- LLMs/RAG/Agents: Prompting/JSON/schema I/O, hybrid BM25+ANN + rerank, span-grounded citations, LLM-judge hygiene in CI, prompt/retriever/index versioning, tool-use with approval gates, deterministic parsers; eval-pack evolution, safe rollback, cost/latency/token SLOs, judge/reranker monitoring, cache taxonomy; leads SFT/PEFT/distillation, SLM-first strategies, model/index/prompt fleet mgmt, joint rollback across model+prompt+index+judge/policy; safety/governance programs, model/index fleet strategy, content provenance policy, org-level budget/vendor strategy.
- Vision/perception: Leads real-time perception stacks at scale, MLLM integrations, multi-camera QoS; org-wide perception roadmap/governance, camera/system standards/audits.

#### Step 2: Specialize in Edge, Search, and Advanced Domains
- Edge/on-device: Model conversion (ONNX/Core ML/TFLite/gguf), PTQ, runtime (NNAPI/CoreML/Metal/WebGPU), offline modes, acceptance tests (p95 ≤ X ms); leads edge program across platforms, telemetry/remote config, staged rollouts, secure model update/signing; org edge/SLM strategy, supplier silicon/NPU partnership input, field ops/compliance strategy.
- Search/retrieval: Leads retriever/reranker architectures, budgeted retrieval, versioning strategy/rollback, marketplace of indices across products; org discovery/search strategy, governance of data/indices/policy.
- De-emphasized: RL and unrelated domains (awareness only); running company-wide Airflow/Spark/feature stores (not part of role).

#### Step 3: Lead Governance, Strategy, and Full Integration
- L4→L5: Owns portfolio of services with joint rollback across model+prompt+index+judge/policy, multi-vendor cut-over drills.
- L5→L6: Sets org-wide trust/safety posture (eval packs, audits, disclosure), enforces via gates.
- Private inference (TEEs/PIR/FHE/MPC awareness).
- Stack: Languages (TypeScript/Next.js for agent UIs); data (Kafka/Kinesis/PubSub, CDC, Airflow/Prefect/Dagster awareness); modeling (JAX awareness); GenAI (CrewAI/AutoGen/Guidance awareness); serving (KServe/BentoML/Seldon, Helm, Argo awareness).
- Project: Design and lead an org-wide AI service portfolio (e.g., SLM-vision agent system), including fleet mgmt, governance audits, multi-region DR, and strategy docs for safety/budgets/vendors.